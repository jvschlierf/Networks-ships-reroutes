{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('Sim_mod': conda)"
  },
  "interpreter": {
   "hash": "c657ab3134d4d36fa6659201da87ce22dbf2c2b56e557e7a3dd81d87fd1e5788"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Data Preparation\n",
    "\n",
    "This file contains the code used to prepare the data. \n",
    "Since the preprocessing (especially the blockage identification using the Haversine Algorithm) are computationally heavy, the outputs from this process are included in the Data/Input folder.\n",
    "Along with it is the original data that was used as input here. \n",
    "\n",
    "The original input was:\n",
    "- ports.csv (Containing a reference, name & position of ports)\n",
    "- routes.csv (Containing the AIS data for routes with position, used to identify which chokepoint (if any) a route passes)\n",
    "- distances.csv (Containing data on previous & next port as well as distance used for network construction)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas\n",
    "import math\n",
    "\n",
    "data_path = 'Data/Input/' #set to wherever the data files are, will be used on every input\n",
    "ports_df = pd.read_csv((data_path +'ports.csv'))\n",
    "distances_df = pd.read_csv((data_path + 'distances.csv'))\n",
    "routes_df = pd.read_csv((data_path + 'routes.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Haversine:\n",
    "    '''\n",
    "    use the haversine class to calculate the distance between\n",
    "    two lon/lat coordnate pairs.\n",
    "    output distance available in kilometers, meters, miles, and feet.\n",
    "    example usage: Haversine([lon1,lat1],[lon2,lat2]).feet\n",
    "    \n",
    "    '''\n",
    "    def __init__(self,coord1,coord2):\n",
    "        lon1,lat1=coord1\n",
    "        lon2,lat2=coord2\n",
    "        \n",
    "        R=6371000                               # radius of Earth in meters\n",
    "        phi_1=math.radians(lat1)\n",
    "        phi_2=math.radians(lat2)\n",
    "\n",
    "        delta_phi=math.radians(lat2-lat1)\n",
    "        delta_lambda=math.radians(lon2-lon1)\n",
    "\n",
    "        a=math.sin(delta_phi/2.0)**2+\\\n",
    "           math.cos(phi_1)*math.cos(phi_2)*\\\n",
    "           math.sin(delta_lambda/2.0)**2\n",
    "        c=2*math.atan2(math.sqrt(a),math.sqrt(1-a))\n",
    "        \n",
    "        self.meters=R*c                         # output distance in meters\n",
    "        self.km=self.meters/1000.0              # output distance in kilometers\n",
    "        self.miles=self.meters*0.000621371      # output distance in miles\n",
    "        self.feet=self.miles*5280               # output distance in feet\n",
    "\n",
    "if __name__ == \"__Haversine__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_detect(routes, chokepoints):\n",
    "    \"\"\"\n",
    "    Detect whether there is a point on the route that goes close to the Chokepoint. \n",
    "    Returns the original route dataframe with distance to the chokepoint and whether it is affected by that chokepoint closure.\n",
    "    \"\"\"\n",
    "\n",
    "    for chokepoint in chokepoints.index:\n",
    "        routes[\"dist_to_{}\".format(chokepoint)] = routes.apply(lambda x: Haversine([x.lon,x.lat], [chokepoints.loc[chokepoint][\"Longitude\"], chokepoints.loc[chokepoint][\"Latitude\"]]).km, axis=1)\n",
    "        routes[\"affected_by_{}\".format(chokepoint)] = np.where(routes[\"dist_to_{}\".format(chokepoint)] <= chokepoints.loc[chokepoint][\"dist_threshold\"], True, False)\n",
    "\n",
    "    return routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chokepoints = pd.DataFrame({'Chokepoint': [\"suez\", \"said\", \"gibraltar\", \"malacca\", \"dover\",  \"balboa\", \"colon\", \"hormuz\" ],\n",
    "                            'Latitude': [29.957175, 31.25366, 35.982003, 3.142137, 51.027659, 8.932536, 9.332827, 26.669853], \n",
    "                            'Longitude':[32.583561, 32.336550,-5.452054, 100.548439, 1.483395, -79.556954, -79.929265, 56.509156],\n",
    "                            'dist_threshold': [15, 15,15, 45, 40, 15, 15, 45,]}).set_index(\"Chokepoint\") #create a dataframe with the chokepoint, lat/lon of a selected point in that chokepoint and a distance threshold (in km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code is commented out since run time is ~12hrs. Output file is included in Data\n",
    "\"\"\"\n",
    "#  r = route_detect(routes_df, chokepoints)\n",
    "# df = r.groupby([\"prev_port\", \"next_port\"], as_index=False).agg({'affected_by_suez':'max','affected_by_said':'max','affected_by_gibraltar':'max','affected_by_malacca':'max','affected_by_dover':'max', 'affected_by_balboa':'max','affected_by_colon':'max','affected_by_hormuz':'max'})\n",
    "# df.to_csv((data_path + 'Route Blockages/'+ \"route_blockage.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_blockages = pd.read_csv((data_path + 'Route Blockages/' + 'route_blockage.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Since we are building a Singlegraph, not a Multigraph, we want to ensure we take the f\n",
    "# clean_distances = distances.sort_values(by=['prev_port', 'next_port', 'distance'])\n",
    "# clean_distances.drop_duplicates(subset=['prev_port', 'next_port'], keep='first', inplace=True, ignore_index=True)\n",
    "# clean_distances.to_csv((data_path + 'clean_distances.csv'))\n"
   ]
  },
  {
   "source": [
    "## Separation by Blockage\n",
    "We separate by blockage and save the output files to use them to cut the Graph in our ABM "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_distances = pd.read_csv((data_path + 'clean_distances.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_blockages_gib =  route_blockages[route_blockages['affected_by_gibraltar']==True]\n",
    "route_blockages_mal =  route_blockages[route_blockages['affected_by_malacca']==True]\n",
    "route_blockages_dov =  route_blockages[route_blockages['affected_by_dover']==True]\n",
    "route_blockages_suez =  route_blockages[route_blockages['affected_by_suez']==True]\n",
    "route_blockages_horm =  route_blockages[route_blockages['affected_by_hormuz']==True]\n",
    "route_blockages_pan = route_blockages[route_blockages['affected_by_panama']==True]\n",
    "\n",
    "route_blockages_horm = route_blockages_horm[[\"prev_port\", \"next_port\"]]\n",
    "route_blockages_gib =  route_blockages_gib[[\"prev_port\", \"next_port\"]]\n",
    "route_blockages_mal =  route_blockages_mal[[\"prev_port\", \"next_port\"]]\n",
    "route_blockages_dov =  route_blockages_dov[[\"prev_port\", \"next_port\"]]\n",
    "route_blockages_suez =  route_blockages_suez[[\"prev_port\", \"next_port\"]]\n",
    "route_blockages_pan = route_blockages_pan[[\"prev_port\", \"next_port\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_blockages_total = pd.concat([route_blockages_horm, route_blockages_gib, route_blockages_mal, route_blockages_dov, route_blockages_suez, route_blockages_pan])\n",
    "route_blockages_total = route_blockages_total.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Commented out for your convenience, files are included\n",
    "\"\"\"\n",
    "# route_blockages_horm.to_csv((data_path + 'Route Blockages/' + 'route_blockages_horm.csv'))\n",
    "# route_blockages_mal.to_csv((data_path + 'Route Blockages/' + 'route_blockages_mal.csv'))\n",
    "# route_blockages_dov.to_csv((data_path + 'Route Blockages/' + 'route_blockages_dov.csv'))\n",
    "# route_blockages_suez.to_csv((data_path + 'Route Blockages/' + 'route_blockages_suez.csv'))\n",
    "# route_blockages_gib.to_csv((data_path + 'Route Blockages/' + 'route_blockages_gib.csv'))\n",
    "# route_blockages_pan.to_csv((data_path + 'Route Blockages/' + 'route_blockages_pan.csv'))\n",
    "# route_blockages_total.to_csv((data_path + 'Route Blockages/' + 'route_blockages_total.csv'))"
   ]
  }
 ]
}